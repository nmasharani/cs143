README file for Programming Assignment 1 (C++ edition)
=====================================================

Your directory should now contain the following files:

 Makefile        -> [course dir]/src/PA1/Makefile
 README
 cool.flex
 test.cl
 lextest.cc      -> [course dir]/src/PA1/lextest.cc
 mycoolc         -> [course dir]/src/PA1/mycoolc
 stringtab.cc    -> [course dir]/src/PA1/stringtab.cc
 utilities.cc    -> [course dir]/src/PA1/utilities.cc
 handle_flags.cc -> [course dir]/src/PA1/handle_flags.cc
 *.d             dependency files
 *.*             other generated files

The include (.h) files for this assignment can be found in 
[course dir]/include/PA1

	The Makefile contains targets for compiling and running your
	program. DO NOT MODIFY.

	The README contains this info. Part of the assignment is to fill
	the README with the write-up for your project. You should
	explain design decisions, explain why your code is correct, and
	why your test cases are adequate. It is part of the assignment
	to clearly and concisely explain things in text as well as to
	comment your code. Just edit this file.

	cool.flex is a skeleton file for the specification of the
	lexical analyzer. You should complete it with your regular
	expressions, patterns and actions. 

	test.cl is a COOL program that you can test the lexical
	analyzer on. It contains some errors, so it won't compile with
	coolc. However, test.cl does not exercise all lexical
	constructs of COOL and part of your assignment is to rewrite
	test.cl with a complete set of tests for your lexical analyzer.

	cool-parse.h contains definitions that are used by almost all parts
	of the compiler. DO NOT MODIFY.

	stringtab.{cc|h} and stringtab_functions.h contains functions
        to manipulate the string tables.  DO NOT MODIFY.

	utilities.{cc|h} contains functions used by the main() part of
	the lextest program. You may want to use the strdup() function
	defined in here. Remember that you should not print anything
	from inside cool.flex! DO NOT MODIFY.

	lextest.cc contains the main function which will call your
	lexer and print out the tokens that it returns.  DO NOT MODIFY.

	mycoolc is a shell script that glues together the phases of the
	compiler using Unix pipes instead of statically linking code.  
	While inefficient, this architecture makes it easy to mix and match
	the components you write with those of the course compiler.
	DO NOT MODIFY.	

        cool-lexer.cc is the scanner generated by flex from cool.flex.
        DO NOT MODIFY IT, as your changes will be overritten the next
        time you run flex.

 	The *.d files are automatically generated Makefiles that capture
 	dependencies between source and header files in this directory.
 	These files are updated automatically by Makefile; see the gmake
 	documentation for a detailed explanation.

Instructions
------------

	To compile your lextest program type:

	% make lexer

	Run your lexer by putting your test input in a file 'foo.cl' and
	run the lextest program:

	% ./lexer foo.cl

	To run your lexer on the file test.cl type:

	% make dotest

	If you think your lexical analyzer is correct and behaves like
	the one we wrote, you can actually try 'mycoolc' and see whether
	it runs and produces correct code for any examples.
	If your lexical analyzer behaves in an
	unexpected manner, you may get errors anywhere, i.e. during
	parsing, during semantic analysis, during code generation or
	only when you run the produced code on spim. So beware.

	To turn in your work type:

	% make submit

	Running "submit" will collect the files cool.flex, test.cl,
	README, and test.output. Don't forget to edit the README file to
	include your write-up, and to write your own test cases in
	test.cl.

 	You may turn in the assignment as many times as you like.
	However, only the last version will be retained for
	grading.

	If you change architectures you must issue

	% make clean

	when you switch from one type of machine to the other.
	If at some point you get weird errors from the linker,	
	you probably forgot this step.

	GOOD LUCK!

---8<------8<------8<------8<---cut here---8<------8<------8<------8<---

Write-up for PA1
----------------
user: lpappas9
user: nisham

DESIGN DECISIONS

One of the biggest and most challenging decisions we made was the 
choice to use exclusive start conditions for strings and nested comments
instead of developing regexes for them. For nested comments, the idea of 
using start conditions was fairly natural, given that we had to balance open
and closed parenthesis and that regexes are not capable of doing so. However, 
for strings, we played around a lot with the usage of regexes before choosing to
use start conditions instead. We decided to use start conditions because the 
rules for strings are incredibly complicated, and we thought it would be easier
for us to design and easier for others to understand if we used start 
conditions. Furthermore, we also realized that, if we used start conditions, 
we could replace escaped characters as we went, which reduced the amount of 
complicated post-processing we had to do to our string. This decision was also
partially inspired by some code we found in the Flex manual that matched 
C-strings using start conditions
(http://flex.sourceforge.net/manual/Start-Conditions.html#Start-Conditions). 

For the rest of the tokens, we chose to use traditional regular 
expressions, as we felt that we did not need the complexity of start conditions
as we had no balancing to do or changes to make to the lexemes.

Note, the error reporting in strings is a challenging issue. We designed strings with 
only a single start tag, to indicate that we are in a string, therefore all 
error handling must happen in the logic for the rules. The advantage of this 
approach is that there are naturally less rules, which makes the comprehension
of the regexes easier. The disadvantage is that it makes the logic of the rule
more involved. 

CORRECTNESS 

Our code contains all the rules for matching tokens as described in section 10
of the Cool manual. Furthermore, our code has produced the same output as the
reference compiler on all of the tests we have run (see the below section 
for details on testing).

We address every case that is listed in the assignment and in the manual. We also
checked every piazza post in order to test for the cases introduced there. 

Our lexer output matches that of lexer.out for every one of our test cases
except for the line number of a null error in string. In this case, we indicate line
number of end of string. The assignment does not specify which is correct. Piazza
indicates that either reporting is fine. Also confirmed with Professor Aiken and TA's 
that our line number reporting in this case is fine. The other case where we don't 
match is the content of some of the error statements. In the assignment, the reported
error statements do not have period. In the reference output, they do have periods. We
followed the assignment and do not have periods, so the diff catches this as well. 

TESTING

Our test cases include both actual Cool code and fragments of code that are both
valid and invalid. A significant portion of our testing code is dedicated to
testing strings and nested comments, which are the two most compilcated parts 
of our lexer. We tested valid inputs as well as invalid inputs, and compared our
outputs to those of the reference compiler. To test the remainder of our rules, 
we used actual Cool code that contained examples of where all of our rules 
should apply. Our tests can be found in test2.cl and in the Testfiles directory.

Note, our test files are named to indicate the purpose of each test. We wrote shell 
scripts to iterate over every one of the files in the test files directory and 
compare our lexer output to that of the sample. This allowed for easier and 
faster testing. 

Our tests cases address every one of the edge conditions addressed in the assignment and
the manual, and we also address every condition raised on piazza as of 14:16:15 April
17th. 